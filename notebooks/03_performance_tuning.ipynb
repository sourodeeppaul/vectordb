{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Tuning Guide\n",
    "\n",
    "This notebook covers advanced performance tuning techniques for VectorDB.\n",
    "\n",
    "## Topics Covered\n",
    "\n",
    "1. Index parameter optimization\n",
    "2. Batch operations\n",
    "3. Memory management\n",
    "4. Query optimization\n",
    "5. Hardware considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from vectordb import VectorDatabase\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "print(f\"Initial memory usage: {get_memory_usage():.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. HNSW Parameter Tuning\n",
    "\n",
    "HNSW has three main parameters:\n",
    "\n",
    "- **M**: Number of connections per node (affects memory and recall)\n",
    "- **ef_construction**: Search depth during build (affects build time and recall)\n",
    "- **ef_search**: Search depth during query (affects query time and recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "N_VECTORS = 50000\n",
    "DIMENSION = 128\n",
    "\n",
    "print(f\"Generating {N_VECTORS} vectors...\")\n",
    "data = np.random.randn(N_VECTORS, DIMENSION).astype(np.float32)\n",
    "queries = np.random.randn(100, DIMENSION).astype(np.float32)\n",
    "ids = [f\"vec_{i}\" for i in range(N_VECTORS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different M values\n",
    "m_values = [8, 16, 32, 48]\n",
    "m_results = []\n",
    "\n",
    "print(\"\\nTesting different M values:\")\n",
    "print(f\"{'M':>6} {'Build (s)':>12} {'Memory (MB)':>14} {'Search (ms)':>12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for m in m_values:\n",
    "    mem_before = get_memory_usage()\n",
    "    \n",
    "    db = VectorDatabase()\n",
    "    collection = db.create_collection(\n",
    "        name=\"test\",\n",
    "        dimension=DIMENSION,\n",
    "        index_type=\"hnsw\",\n",
    "        index_params={\"M\": m, \"ef_construction\": 100}\n",
    "    )\n",
    "    \n",
    "    # Build\n",
    "    start = time.time()\n",
    "    collection.add(data, ids=ids)\n",
    "    build_time = time.time() - start\n",
    "    \n",
    "    mem_after = get_memory_usage()\n",
    "    \n",
    "    # Search\n",
    "    start = time.time()\n",
    "    for q in queries[:10]:\n",
    "        collection.search(q, k=10)\n",
    "    search_time = (time.time() - start) / 10 * 1000  # ms per query\n",
    "    \n",
    "    m_results.append({\n",
    "        'M': m,\n",
    "        'build_time': build_time,\n",
    "        'memory': mem_after - mem_before,\n",
    "        'search_time': search_time\n",
    "    })\n",
    "    \n",
    "    print(f\"{m:>6} {build_time:>12.2f} {mem_after - mem_before:>14.1f} {search_time:>12.2f}\")\n",
    "    \n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different ef_construction values\n",
    "ef_construction_values = [50, 100, 200, 400]\n",
    "\n",
    "print(\"\\nTesting different ef_construction values (M=16):\")\n",
    "print(f\"{'ef_c':>8} {'Build (s)':>12} {'Recall':>10}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for ef_c in ef_construction_values:\n",
    "    db = VectorDatabase()\n",
    "    collection = db.create_collection(\n",
    "        name=\"test\",\n",
    "        dimension=DIMENSION,\n",
    "        index_type=\"hnsw\",\n",
    "        index_params={\"M\": 16, \"ef_construction\": ef_c}\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    collection.add(data, ids=ids)\n",
    "    build_time = time.time() - start\n",
    "    \n",
    "    # Note: In production, you'd compute actual recall against ground truth\n",
    "    print(f\"{ef_c:>8} {build_time:>12.2f} {'~varies':>10}\")\n",
    "    \n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HNSW Tuning Guidelines\n",
    "\n",
    "| Use Case | M | ef_construction | ef_search |\n",
    "|----------|---|-----------------|------------|\n",
    "| Low memory | 8-12 | 50-100 | 20-50 |\n",
    "| Balanced | 16-24 | 100-200 | 50-100 |\n",
    "| High recall | 32-48 | 200-500 | 100-500 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch Operations\n",
    "\n",
    "Batching operations significantly improves throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare single vs batch insertions\n",
    "db = VectorDatabase()\n",
    "collection = db.create_collection(name=\"batch_test\", dimension=DIMENSION)\n",
    "\n",
    "test_vectors = data[:5000]\n",
    "test_ids = ids[:5000]\n",
    "\n",
    "# Single insertions\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    collection.add(test_vectors[i:i+1], ids=[test_ids[i]])\n",
    "single_time = time.time() - start\n",
    "single_rate = 1000 / single_time\n",
    "\n",
    "db.delete_collection(\"batch_test\")\n",
    "collection = db.create_collection(name=\"batch_test\", dimension=DIMENSION)\n",
    "\n",
    "# Batch insertions\n",
    "batch_sizes = [10, 100, 500, 1000]\n",
    "print(f\"{'Batch Size':>12} {'Time (s)':>12} {'Rate (vec/s)':>14} {'Speedup':>10}\")\n",
    "print(\"-\" * 52)\n",
    "print(f\"{'1 (single)':>12} {single_time:>12.3f} {single_rate:>14.1f} {'1.0x':>10}\")\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    db.delete_collection(\"batch_test\")\n",
    "    collection = db.create_collection(name=\"batch_test\", dimension=DIMENSION)\n",
    "    \n",
    "    start = time.time()\n",
    "    for i in range(0, 1000, batch_size):\n",
    "        end = min(i + batch_size, 1000)\n",
    "        collection.add(test_vectors[i:end], ids=test_ids[i:end])\n",
    "    batch_time = time.time() - start\n",
    "    batch_rate = 1000 / batch_time\n",
    "    speedup = batch_rate / single_rate\n",
    "    \n",
    "    print(f\"{batch_size:>12} {batch_time:>12.3f} {batch_rate:>14.1f} {speedup:>9.1f}x\")\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch search performance\n",
    "db = VectorDatabase()\n",
    "collection = db.create_collection(name=\"search_test\", dimension=DIMENSION, index_type=\"hnsw\")\n",
    "collection.add(data[:10000], ids=ids[:10000])\n",
    "\n",
    "n_queries = 100\n",
    "test_queries = queries[:n_queries]\n",
    "\n",
    "# Individual searches\n",
    "start = time.time()\n",
    "for q in test_queries:\n",
    "    collection.search(q, k=10)\n",
    "individual_time = time.time() - start\n",
    "\n",
    "# Batch search\n",
    "start = time.time()\n",
    "collection.search_batch(test_queries, k=10)\n",
    "batch_time = time.time() - start\n",
    "\n",
    "print(f\"\\nSearch Performance ({n_queries} queries):\")\n",
    "print(f\"  Individual: {individual_time:.3f}s ({n_queries/individual_time:.0f} QPS)\")\n",
    "print(f\"  Batch:      {batch_time:.3f}s ({n_queries/batch_time:.0f} QPS)\")\n",
    "print(f\"  Speedup:    {individual_time/batch_time:.2f}x\")\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare memory usage of different configurations\n",
    "configs = [\n",
    "    {\"name\": \"Flat\", \"index_type\": \"flat\", \"index_params\": {}},\n",
    "    {\"name\": \"HNSW (M=16)\", \"index_type\": \"hnsw\", \"index_params\": {\"M\": 16}},\n",
    "    {\"name\": \"HNSW (M=32)\", \"index_type\": \"hnsw\", \"index_params\": {\"M\": 32}},\n",
    "    {\"name\": \"IVF (100 clusters)\", \"index_type\": \"ivf\", \"index_params\": {\"n_clusters\": 100}},\n",
    "]\n",
    "\n",
    "print(f\"Memory usage for {N_VECTORS} vectors ({DIMENSION}D):\")\n",
    "print(f\"Raw vector size: {N_VECTORS * DIMENSION * 4 / 1024 / 1024:.1f} MB\")\n",
    "print()\n",
    "print(f\"{'Configuration':<25} {'Total (MB)':>12} {'Overhead':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "raw_size = N_VECTORS * DIMENSION * 4 / 1024 / 1024\n",
    "\n",
    "for config in configs:\n",
    "    mem_before = get_memory_usage()\n",
    "    \n",
    "    db = VectorDatabase()\n",
    "    collection = db.create_collection(\n",
    "        name=\"mem_test\",\n",
    "        dimension=DIMENSION,\n",
    "        index_type=config[\"index_type\"],\n",
    "        index_params=config[\"index_params\"]\n",
    "    )\n",
    "    collection.add(data, ids=ids)\n",
    "    \n",
    "    mem_after = get_memory_usage()\n",
    "    mem_used = mem_after - mem_before\n",
    "    overhead = (mem_used / raw_size - 1) * 100\n",
    "    \n",
    "    print(f\"{config['name']:<25} {mem_used:>12.1f} {overhead:>9.0f}%\")\n",
    "    \n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Reduction Strategies\n",
    "\n",
    "1. **Use Product Quantization**: Compresses vectors to 1/4 - 1/16 of original size\n",
    "2. **Lower M for HNSW**: Reduces graph storage overhead\n",
    "3. **Use disk-based storage**: For datasets larger than RAM\n",
    "4. **Reduce vector dimension**: Use dimensionality reduction (PCA, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup test collection with metadata\n",
    "db = VectorDatabase()\n",
    "collection = db.create_collection(\n",
    "    name=\"query_opt\",\n",
    "    dimension=DIMENSION,\n",
    "    index_type=\"hnsw\"\n",
    ")\n",
    "\n",
    "# Add vectors with metadata\n",
    "categories = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "metadata = [\n",
    "    {\"category\": categories[i % 5], \"value\": i % 100, \"active\": i % 2 == 0}\n",
    "    for i in range(N_VECTORS)\n",
    "]\n",
    "collection.add(data, ids=ids, metadata=metadata)\n",
    "\n",
    "print(f\"Added {collection.count()} vectors with metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare filtered vs unfiltered search\n",
    "query = queries[0]\n",
    "n_iterations = 50\n",
    "\n",
    "# Unfiltered\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    collection.search(query, k=10)\n",
    "unfiltered_time = (time.time() - start) / n_iterations * 1000\n",
    "\n",
    "# Simple filter (matches 20% of data)\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    collection.search(query, k=10, filter={\"category\": \"A\"})\n",
    "simple_filter_time = (time.time() - start) / n_iterations * 1000\n",
    "\n",
    "# Complex filter\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    collection.search(query, k=10, filter={\n",
    "        \"$and\": [\n",
    "            {\"category\": {\"$in\": [\"A\", \"B\"]}},\n",
    "            {\"value\": {\"$gte\": 50}},\n",
    "            {\"active\": True}\n",
    "        ]\n",
    "    })\n",
    "complex_filter_time = (time.time() - start) / n_iterations * 1000\n",
    "\n",
    "print(\"Query Performance Comparison:\")\n",
    "print(f\"  Unfiltered:     {unfiltered_time:.2f} ms\")\n",
    "print(f\"  Simple filter:  {simple_filter_time:.2f} ms ({simple_filter_time/unfiltered_time:.1f}x)\")\n",
    "print(f\"  Complex filter: {complex_filter_time:.2f} ms ({complex_filter_time/unfiltered_time:.1f}x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact of k on query time\n",
    "k_values = [1, 10, 50, 100, 500]\n",
    "\n",
    "print(\"\\nQuery time vs k:\")\n",
    "print(f\"{'k':>8} {'Time (ms)':>12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "for k in k_values:\n",
    "    start = time.time()\n",
    "    for _ in range(n_iterations):\n",
    "        collection.search(query, k=k)\n",
    "    query_time = (time.time() - start) / n_iterations * 1000\n",
    "    print(f\"{k:>8} {query_time:>12.2f}\")\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Optimization Tips\n",
    "\n",
    "1. **Use appropriate k**: Only retrieve as many results as needed\n",
    "2. **Optimize filters**: More selective filters = faster queries\n",
    "3. **Batch queries**: Use `search_batch` for multiple queries\n",
    "4. **Tune ef_search**: Lower values for speed, higher for recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Persistence and I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Create persistent database\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "db_path = f\"{temp_dir}/vectordb\"\n",
    "\n",
    "# Write test\n",
    "print(\"Testing persistence performance...\")\n",
    "db = VectorDatabase(storage_path=db_path)\n",
    "collection = db.create_collection(\n",
    "    name=\"persist_test\",\n",
    "    dimension=DIMENSION,\n",
    "    index_type=\"hnsw\"\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "collection.add(data[:10000], ids=ids[:10000])\n",
    "db.close()  # Triggers save\n",
    "write_time = time.time() - start\n",
    "\n",
    "# Read test\n",
    "start = time.time()\n",
    "db = VectorDatabase(storage_path=db_path)\n",
    "collection = db.get_collection(\"persist_test\")\n",
    "load_time = time.time() - start\n",
    "\n",
    "# Verify\n",
    "results = collection.search(queries[0], k=5)\n",
    "\n",
    "print(f\"\\nPersistence Performance (10,000 vectors):\")\n",
    "print(f\"  Write time: {write_time:.2f}s\")\n",
    "print(f\"  Load time:  {load_time:.2f}s\")\n",
    "print(f\"  Loaded vectors: {collection.count()}\")\n",
    "\n",
    "db.close()\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices Summary\n",
    "\n",
    "### Index Selection\n",
    "- **< 10k vectors**: Use Flat\n",
    "- **10k - 1M vectors**: Use HNSW with M=16-24\n",
    "- **> 1M vectors**: Consider IVF-PQ hybrid\n",
    "\n",
    "### HNSW Tuning\n",
    "- Start with M=16, ef_construction=100\n",
    "- Increase M for higher recall (at memory cost)\n",
    "- Tune ef_search at query time for speed/recall tradeoff\n",
    "\n",
    "### Operations\n",
    "- Batch insertions: 100-1000 vectors per batch\n",
    "- Batch queries: Use search_batch for multiple queries\n",
    "- Pre-filter when possible to reduce search space\n",
    "\n",
    "### Memory\n",
    "- Monitor memory usage with large datasets\n",
    "- Use PQ for memory-constrained environments\n",
    "- Consider disk-based storage for very large datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}